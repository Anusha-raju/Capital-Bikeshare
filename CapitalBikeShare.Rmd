---
title: "Analysis and modelling on the Capital Bikeshare data"
author: "[Anusha Umashankar](https://github.com/Anusha-raju)| [Aidan Carlisle](https://github.com/acarlisle8)| [Rachel Thomas](https://github.com/releered)| [Sayan Patra](https://github.com/Sayanpatraa) - Group 8 "
# date: "today"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
---

<style>

h1 { color: #800000; }  

h2 { color: #000080; } 
h3 { color: #004d4d; } 

</style>

```{r init, include=F}
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course. 
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
# Once installed, load the library.
library(ezids)
```


```{r setup, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
knitr::opts_chunk$set(warning = F, results = "markup", message = F)
options(scientific=T, digits = 3) 
```

```{r include=FALSE}
library(dplyr)
library(ggplot2)
library(tidyr)
library(geosphere)
library(tidygeocoder)
library(caret)
library(lattice)
library(lubridate)
library(rattle)
library(rpart)
library(rpart.plot)
library(car)
library(pROC)
```



capital_bikeshare <- X202410_capitalbikeshare_tripdata


```{r}
file_path <- file.path(getwd(), "X202410_capitalbikeshare_tripdata.csv")
capital_bikeshare <- read.csv(file_path, header = TRUE)
```

The dataset contains the following attributes.
```{r}
colnames(capital_bikeshare)
```

# Preprocessing:

1. 
There are data points where information like start_station_name & id , end_station_name & id , and end_lat and end_lng are missing. 
`r colSums(is.na(capital_bikeshare))`.

Dropping the `r colSums(is.na(capital_bikeshare))["end_lat"]` rows as they consitute only `r colSums(is.na(capital_bikeshare))["end_lat"]/nrow(capital_bikeshare)*100` percent of the dataset. 

```{r}
capital_bikeshare <- capital_bikeshare %>%
  filter(!is.na(end_lat))
```


2. Reverse Geocoding: Converting Latitude and Longitude Coordinates to Addresses
```{r}
#capital_bikeshare$start_addy<-reverse_geocode(capital_bikeshare, lat = "start_lat", long = "start_lng")
#capital_bikeshare$end_addy<-reverse_geocode(capital_bikeshare, lat = "end_lat", long = "end_lng")
```

3. `Distance (in miles)` is derived using the latitude and longitude coordinates of the start and end points. The `distVincentySphere()` function from the `geosphere` package in R is used to compute the geodesic distance, factoring in the Earth's curvature.

```{r}
# Calculate the distance for each row and add it as a new column (in miles)
capital_bikeshare$distance_miles <- mapply(function(lat1, lon1, lat2, lon2) {
  distVincentySphere(c(lon1, lat1), c(lon2, lat2)) / 1000 * 0.621371  
}, capital_bikeshare$start_lat, capital_bikeshare$start_lng, capital_bikeshare$end_lat, capital_bikeshare$end_lng)
```


4. Calculated the total ride time by subtracting `started_at` from `ended_at.`
```{r}
capital_bikeshare$started_at <- as.POSIXct(capital_bikeshare$started_at, format="%Y-%m-%d %H:%M:%OS")
capital_bikeshare$ended_at <- as.POSIXct(capital_bikeshare$ended_at, format="%Y-%m-%d %H:%M:%OS")
capital_bikeshare$ride_duration_mins <- as.numeric(difftime(capital_bikeshare$ended_at, capital_bikeshare$started_at, units = "mins"))
```


5. Categorizing Rides by Time Period.
```{r}
get_time_period <- function(time) {
  hour <- as.numeric(format(time, "%H"))
  
  if (hour >= 4 & hour < 6) {
    return("Early Morning")
  } else if (hour >= 6 & hour < 9) {
    return("Morning")
  } else if (hour >= 9 & hour < 12) {
    return("Mid-Morning")
  } else if (hour >= 12 & hour < 13) {
    return("Midday")
  } else if (hour >= 13 & hour < 17) {
    return("Afternoon")
  } else if (hour >= 17 & hour < 19) {
    return("Evening")
  } else if (hour >= 19 & hour < 21) {
    return("Late Evening")
  } else {
    return("Night")
  }
}


capital_bikeshare <- capital_bikeshare %>%
  mutate(
    start_time_period = sapply(started_at, get_time_period),
  )
```

6. Categorizing rides by weekday vs weekend
```{r}
capital_bikeshare$day_type <- ifelse(wday(capital_bikeshare$started_at) %in% c(1, 7), "weekend", "weekday")

```



**The following attributes are considered for further Exploratory Data Analysis (EDA):**

```{r}
#filtered_capital_bikeshare <- capital_bikeshare %>% select(rideable_type,member_casual,distance_miles,ride_duration_mins, start_time_period, start_addy, end_addy)
filtered_capital_bikeshare <- capital_bikeshare %>% select(rideable_type,member_casual,distance_miles,ride_duration_mins, start_time_period,day_type)
```

*Factorizing 'rideable_type', 'member_casual', 'start_time_period', and 'day_type' variables:*
```{r}
filtered_capital_bikeshare$rideable_type <- as.factor(filtered_capital_bikeshare$rideable_type) 
filtered_capital_bikeshare$member_casual <- as.factor(filtered_capital_bikeshare$member_casual)
filtered_capital_bikeshare$start_time_period <- as.factor(filtered_capital_bikeshare$start_time_period)
filtered_capital_bikeshare$day_type <- as.factor(filtered_capital_bikeshare$day_type)
```

*Removing outliers*
```{r}
#for distance_miles
z_scores_DM <- scale(filtered_capital_bikeshare$distance_miles)
outliers <- which(abs(z_scores_DM) > 3)
capital_bikeshare_clean <- filtered_capital_bikeshare[-outliers, ]

# for ride_duration_mins
z_scores_RDM <- scale(capital_bikeshare_clean$ride_duration_mins)
outliers <- which(abs(z_scores_RDM) > 2)
capital_bikeshare_clean <- capital_bikeshare_clean[-outliers, ]

```

After cleaning the dataset the number of rows are `r nrow(capital_bikeshare_clean)`.

# Visualizations:
*1. Frequency of electric bike versus classic bike rentals*
```{r}

bike_type_counts <- table(capital_bikeshare_clean$rideable_type)

bike_type_df <- as.data.frame(bike_type_counts)

# Create a bar plot to visualize the frequency of bike rentals by type
ggplot(bike_type_df, aes(x = Var1, y = Freq, fill = Var1)) + 
  geom_bar(stat = "identity") + 
  labs(title = "Frequency of Electric Bike vs Classic Bike Rentals",
       x = "Bike Type",
       y = "Frequency of Rentals") + 
  scale_fill_manual(values = c("electric_bike" = "violet", "classic_bike" = "lightblue")) +  
  scale_y_continuous(labels = scales::comma) +
  theme(legend.position = "none")
```

- The higher frequency of electric bike rentals compared to classic bikes in the dataset indicates a preference for electric bikes. 


*2. Distribution of duration rides on electric bike versus classic bike*

```{r}
ggplot(capital_bikeshare_clean, aes(x = ride_duration_mins, fill = rideable_type)) +
  geom_density(alpha = 0.9) +  # Alpha sets the transparency for overlapping areas
  labs(title = "Distribution of Ride Durations for Electric vs Classic Bikes",
       x = "Ride Duration (minutes)",
       y = "Density") +
  scale_fill_manual(values = c("electric_bike" = "red", "classic_bike" = "green")) +  
  theme_minimal()

ggplot(capital_bikeshare_clean, aes(x = ride_duration_mins, fill = rideable_type)) +
  geom_histogram(binwidth = 2, color = "black", alpha = 0.7, position = "dodge") +
  labs(
    title = "Distribution of Ride Duration by Bike Type",
    x = "Ride Duration (Minutes)",
    y = "Frequency"
  ) +
  scale_fill_manual(values = c("electric_bike" = "pink", "classic_bike" = "red")) +
  scale_y_continuous(labels = scales::comma) +
  theme_minimal()

```
Box plot
```{r}
ggplot(capital_bikeshare_clean, aes(x = rideable_type, y = ride_duration_mins, fill = rideable_type)) +
  geom_boxplot() +
  labs(title = "Boxplot of Ride Durations for Electric vs Classic Bikes",
       x = "Bike Type",
       y = "Ride Duration (minutes)") +
  scale_fill_manual(values = c("electric_bike" = "blue", "classic_bike" = "green")) +
  theme_minimal()
```



*3. Frequency of user types*

```{r}
user_type_counts <- table(capital_bikeshare_clean$member_casual)

user_type_df <- as.data.frame(user_type_counts)

ggplot(user_type_df, aes(x = Var1, y = Freq, fill = Var1)) +
  geom_bar(stat = "identity") +
  labs(title = "Frequency of User Types (Member vs Casual)",
       x = "User Type",
       y = "Frequency") +
  scale_fill_manual(values = c("member" = "blue", "casual" = "green")) +  
  theme_minimal()
```



Pie chart:
```{r}
ggplot(user_type_df, aes(x = "", y = Freq, fill = User_Type)) +
  geom_bar(stat = "identity", width = 1) + 
  coord_polar(theta = "y") +  # Convert the bar chart into a pie chart
  labs(title = "Proportion of User Types (Member vs Casual)") +
  scale_fill_manual(values = c("member" = "blue", "casual" = "green")) +
  theme_void() 
```

*4. User Types by Rideable Type*
```{r}
user_bike_counts <- capital_bikeshare_clean %>%
  group_by(member_casual, rideable_type) %>%
  summarise(Freq = n())  

ggplot(user_bike_counts, aes(x = rideable_type, y = Freq, fill = member_casual)) +
  geom_bar(stat = "identity", position = "dodge") +  
  labs(title = "User Type Distribution by Rideable Type",
       x = "Rideable Type (Bike Type)",
       y = "Frequency of Rides") +
  scale_fill_manual(values = c("member" = "pink", "casual" = "yellow")) +  
  theme_minimal()
```

```{r}
ggplot(user_bike_counts, aes(x = rideable_type, y = Freq, fill = member_casual)) +
  geom_bar(stat = "identity", position = "fill") +  
  labs(title = "Proportional User Type Distribution by Rideable Type",
       x = "Rideable Type (Bike Type)",
       y = "Proportion of Rides") +
  scale_fill_manual(values = c("member" = "pink", "casual" = "violet")) +  
  theme_minimal()
```

*5. Member vs Casual bike usage on weekdays vs weekends*
```{r}
ggplot(capital_bikeshare_clean, aes(x = day_type, fill = interaction(member_casual, rideable_type))) +
  geom_bar(position = "dodge") + # Dodged bars for separate member_casual and rideable_type combinations
  labs(title = "Rides by Day Type (Weekday vs Weekend)",
       x = "Day Type",
       y = "Count",
       fill = "Member Type & Rideable Type") +
  theme_minimal()
```

# Predict the rider's class based on trip characteristics
```{r}
riderLogit01 <- glm(member_casual ~ ride_duration_mins, data = capital_bikeshare_clean, family = "binomial")
summary(riderLogit01)

riderLogit02 <- glm(member_casual ~ rideable_type + ride_duration_mins, data = capital_bikeshare_clean, family = "binomial")
summary(riderLogit02)

riderLogit03 <- glm(member_casual ~ rideable_type + ride_duration_mins + day_type, data = capital_bikeshare_clean, family = "binomial")
summary(riderLogit03)

riderLogit04 <- glm(member_casual ~ rideable_type + distance_miles + ride_duration_mins + day_type, data = capital_bikeshare_clean, family = "binomial")
summary(riderLogit04)

riderLogit05 <- glm(member_casual ~ rideable_type * ride_duration_mins, data = capital_bikeshare_clean, family = "binomial")
summary(riderLogit05)

riderLogit06 <- glm(member_casual ~ rideable_type * ride_duration_mins * day_type, data = capital_bikeshare_clean, family = "binomial")
summary(riderLogit06)

riderLogit07 <- glm(member_casual ~ rideable_type * distance_miles * ride_duration_mins * day_type, data = capital_bikeshare_clean, family = "binomial")
summary(riderLogit07)

riderLogit08 <- glm(member_casual ~ rideable_type * distance_miles * ride_duration_mins * day_type * start_time_period, data = capital_bikeshare_clean, family = "binomial")
summary(riderLogit08)

riderLogit09 <- glm(member_casual ~ rideable_type + distance_miles + ride_duration_mins + start_time_period + day_type, data = capital_bikeshare_clean, family = "binomial")
summary(riderLogit09)

riderLogit10 <- glm(member_casual ~ rideable_type * ride_duration_mins + distance_miles + start_time_period + day_type, data = capital_bikeshare_clean, family = "binomial")
summary(riderLogit10)

riderLogit11 <- glm(member_casual ~ distance_miles + rideable_type * ride_duration_mins + day_type, data = capital_bikeshare_clean, family = "binomial")
summary(riderLogit11)
```
AIC10 = 847467
AIC11 = 849314



ANOVA to compare models and select the best
```{r}
anova_result <- anova(riderLogit01, riderLogit02, riderLogit03, riderLogit04, riderLogit05, riderLogit06, riderLogit07, riderLogit08, riderLogit09, test = "Chisq")
print(anova_result)

anova_result <- anova(riderLogit04, riderLogit09, test = "Chisq")
print(anova_result)

anova_result <- anova(riderLogit09, riderLogit10, test = "Chisq")
print(anova_result)

anova_result <- anova(riderLogit04, riderLogit11, test = "Chisq")
print(anova_result)

anova_result <- anova(riderLogit11, riderLogit10, test = "Chisq")
print(anova_result)

anova_result <- anova(riderLogit04, riderLogit10, test = "Chisq")
print(anova_result)
```

```{r}

vif(riderLogit06)
vif(riderLogit04)
vif(riderLogit09)
vif(riderLogit10)
vif(riderLogit11)
```

Model selection started with duration (ride_duration_mins) only and then added bike type (rideable_type) because the decision tree indicated these were the most important variables (see below). Because this data doesn't have many variables, and nearly all were statistically significant in the full model, we elected a stepwise approach to model building, and used ANOVA to compare model quality. Initially Models 4 and 6 had the best reduction in residual deviance for minimal change in degrees of freedom that was statistically significant. However the VIF for model 6 indicated a significant amount of collinearity. So then we compared model 4 to the full model (9), which showed a smaller but still statistically significant reduction to residual deviance. However this model increased the complexity by 7 degrees of freedom. We tried 2 other versions of the model, incorporating the interaction terms between duration and bike type, but with and without the time-of-day (start_time_period). Model10(riderLogit10) including all variables and the interaction between duration and bike type is a better fit when compared via ANOVA to the model without time-of-day, with a statistically significant reduction in residual deviance.

Model 10 - member_casual ~ rideable_type * ride_duration_mins + distance_miles + start_time_period + day_type
```{r}
riderLogit10 <- glm(member_casual ~ rideable_type * ride_duration_mins + distance_miles + start_time_period + day_type, data = capital_bikeshare_clean, family = "binomial")
summary(riderLogit10)
```

The intercept is log-odds 1.52, the log-odds of being a member when the bike is a classic, ride duration and distance are 0, it is the afternoon, and it is a weekday.
Electric bike (verus classic bike) reduces the log-odds of being a member by log-odds 0.631.
Each additional minute of ride duration decrease the log-odds of being a member by 0.062.
Each additional mile of distance ridden increases the log-odds of being a member b 0.12
Evening, Late evening, Mid-morning, Morning increase the log-odds of being a member compared to afternoon. 
Early morning, midday, and Night are not statistically significant at alpha = 0.05.
Weekend rides reduce the likelihood of being a member by log-odds 0.42.
The interaction term indicates that for each additional minute of ride duration on an electric bike, the negative effect of ride duration (see above) on the log-odds of being a member is reduced by 0.04. This means longer rides on electric bikes are less strongly associated with with casual riders on classic bikes.



```{r}
exp(coef(riderLogit10))
```
When the log-odds are exponentiated, we can use the odds ratios (OR) to better understand the impact of the variables.
The intercept is 4.559 meaning the odds of being a member are 4.56 times higher than being a casual user when the bike is a classic, it is a weekday afternoon, and ride duration and distance are 0.
The odds ratio for electric bike is 0.541, meaning riders on electric bikes are 46% (1-0.541) less likely to be members compared to classic bikes.
The odds ratio for duraiton is 0.94, meaning each additional minute of ride duration decreases the odds of being a member by 6% (1-0.94).
The odds ratio for distance is 1.127, meaning each additional mile of distance increases the odds of being a member by 13%.
The odds ratio for morning rides is 1.499, meaning meaning morning rides are 50% more likely to be a member than afternoon rides.
The odds ratio for mid-morning is 1.080, meaning odds of being a member is 8% higher than in the afternoon. 
The odds ratio for evening is 1.098, meaning the odds of being a member is 9.8% higher than in the afternoon.
The odds ratio for late evening is 1.10, meaning the odds of being a memberis 10% higher than in the afternoon.
The odds ratio for weekend rides is 0.568, meaning riders on weekends are 34% (1-0.658) less likely to be members.
The odds ratio for the interaction between bike type and duration is 1.041, meaning each additional minute of ride time on electric bikes increases the odds of being a memebr by 4.1%, which counteracts the negative effect of longer rides overall.

Riders are overall more likely to be memebrs. Electric bikes and longer rides reduce the odds, though less so if the longer ride is on an electric bike. Longer distances, increase the odds of membership, while weekends reduce the odds. Morning rides strongly increase the odds of membership.


```{r}
predicted_probs <- predict(riderLogit10, type = "response")

# Classify based on a threshold of 0.5
predicted_classes <- ifelse(predicted_probs > 0.5, "member", "casual")
predicted_classes <- factor(predicted_classes, levels = c("member", "casual"))
actual_classes <- factor(capital_bikeshare_clean$member_casual, levels = c("member", "casual"))

# Create the confusion matrix
conf_matrix <- confusionMatrix(predicted_classes, actual_classes)

# Print the confusion matrix and statistics
print(conf_matrix)

# Extract accuracy
accuracy <- conf_matrix$overall["Accuracy"]
print(paste("Accuracy:", round(accuracy, 4)))
```

This model is approximately 70% accurate, which is less than the 80% target but decent considering the data for this month is moderately skewed toward members. The No Information Rate (NIR) is 0.675 or 67.5%, which is the accuracy of a naive model always predicting the majority class - this model is statistically significantly better than the NIR. However the Kappa is low, suggesting limited improvement over guessing and Mcnemar's test indicates the model misclassifies disporportionately.Not surprisingly, this model is very sensitive (very good at identifying members), but not very specific (not good at identifying casual riders).


```{r}
predicted_probs10 <- predict(riderLogit10, newdata = capital_bikeshare_clean, type = "response")
roc_curve10 <- roc(response = capital_bikeshare_clean$member_casual, predicted_probs10, levels = c("casual", "member"))
plot(roc_curve, main = "ROC Curve for Logistic Regression Model", col = "blue", lwd = 2)

auc_value10 <- auc(roc_curve10)
print(paste("AUC (ModelMetrics):", round(auc_value10, 4)))

```

The AUC of the model is 0.6337 which is certainly below the target of 0.8.

McFadden R-squared
```{r, echo=TRUE}
riderLogit10pr2 = pR2(riderLogit10)
riderLogit10pr2
```

The McFadden's R-square is 0.0457 or (4.57%), which indicates this model explains a small but meaningful portion of variation in outcome compared to the null model. 
This dataset is a small snapshot of users and may not accurately reflect the true distribution of users of the course of the year. There are many additional factors that likely affect bike usage including time of year/season, weather, when it gets dark. If we applied this model to the entire year, the accuracy may change. But this would likely involve >12 million observations and we lack the computing power to assess that. 

We could have created a subset sample that oversamples the casual riders (the minority class) and undersampled the member riders (the majority class), to create a balanced sample for the the analysis. 


# Model to predict the duration of the ride based on trip characteristics and type of rider.

```{r}

DurationModel <- rpart(ride_duration_mins ~ rideable_type + member_casual + distance_miles + start_time_period, data = capital_bikeshare_clean, method = "anova")

```
*Summary of Duration Model*
```{r}
print(summary(DurationModel))
```



```{r}
rpart.plot(DurationModel, type = 3, extra = 1, main = "Regression Tree for Ride Duration Prediction")

```
Regression tree to predict ride duration in minutes based on rideable_type, member_casual, distance_miles, and start_time_period.


- The root node contains all observations, with an average ride duration of 12.7 minutes.

- The first major split occurs at 1.55 miles for distance_miles. This results in two main branches:

  - For distances less than 1.55 miles, the average ride duration is 9.86 minutes.
  - For distances greater than or equal to 1.55 miles, the average ride duration increases to 20.7 minutes.
  
Key Predictors:

Distance is the primary factor influencing ride duration, with significant splits at 0.871 miles and 2.46 miles.
Rideable type and member status also significantly affect ride duration:
Members tend to have shorter ride durations than casual riders, especially for shorter distances.
Electric bikes are associated with shorter ride durations compared to classic bikes.


For trips with distance_miles < 0.871 and member status = member, the ride duration averages 6.96 minutes.
For casual riders with distance_miles < 0.871, the average ride duration is 10.90 minutes.
For trips with distance_miles ≥ 0.871 and < 2.46, the average duration is 12.6 minutes. Here, electric bike rides average 11.0 minutes, and classic bike rides average 15.2 minutes.
For longer trips with distance_miles ≥ 2.46, the ride duration increases further to 25.8 minutes, with electric bike rides averaging 23.6 minutes and classic bike rides averaging 32.4 minutes.
Model Performance:

The model effectively splits based on the most significant predictors, with the tree having 8 terminal nodes.

```{r}

# Build a tree for member_casual
Member_casual_tree <- rpart(member_casual~ rideable_type + distance_miles + ride_duration_mins + start_time_period + day_type, data = capital_bikeshare_clean, method = "class")

# Plot the tree
tree_vis <- rpart.plot(Member_casual_tree, type = 3, extra = 1, main = "Classification Tree for Member vs Casual Prediction")

# Print the tree
tree_vis

# Print the summary of the tree
printcp(Member_casual_tree)

# K-fold 
fold <- floor(runif(nrow(capital_bikeshare_clean),1,11)) 
  capital_bikeshare_clean$fold <- fold
## the test set is just the first fold 
test.set <- capital_bikeshare_clean[capital_bikeshare_clean$fold == 1,] 
##exclude the first fold from the data here 
train.set <- capital_bikeshare_clean[capital_bikeshare_clean$fold != 1,]
treefit_fold <- rpart(member_casual~ rideable_type + distance_miles + ride_duration_mins + start_time_period+ day_type, data = capital_bikeshare_clean, method = "class")

fancyRpartPlot(treefit_fold)

pred <- predict(treefit_fold, test.set) 

treefit_fold_error = sum(pred-test.set$member_casual)^2
treefit_fold_error <- sum((pred - test.set$member_casual)^2)
pred <- pred[, 2]  # Convert matrix to vector
treefit_fold_error <- sum((pred - test.set$member_casual)^2)
print(treefit_fold_error)





```

```{r}
# Load necessary libraries
library(rpart)
library(rpart.plot)

# K-fold cross-validation
set.seed(123)  # For reproducibility
fold <- floor(runif(nrow(capital_bikeshare_clean), 1, 6)) 
capital_bikeshare_clean$fold <- fold

# Create empty vectors to store results
accuracy_results <- c()

# Loop through each fold (1 to 10)
for (i in 1:5) {
  # Create the test and training sets
  test.set <- capital_bikeshare_clean[capital_bikeshare_clean$fold == i,] 
  train.set <- capital_bikeshare_clean[capital_bikeshare_clean$fold != i,]
  
  # Train the decision tree model
  Member_casual_tree <- rpart(member_casual ~ rideable_type + distance_miles + ride_duration_mins + start_time_period + day_type, 
                              data = train.set, 
                              method = "class")
  
  # Visualize the tree (optional)
  rpart.plot(Member_casual_tree, type = 3, extra = 1, main = "Classification Tree for Member vs Casual Prediction")
  
  # Print the summary of the tree
  printcp(Member_casual_tree)
  
  # Make predictions on the test set
  pred_prob <- predict(Member_casual_tree, test.set, type = "prob")
  
  # Convert probabilities to predicted classes (1 for 'Member', 2 for 'Casual')
  pred_class <- ifelse(pred_prob[, 2] > 0.5, "casual", "member")  # Assuming 2nd class (casual) is the positive class
  
  # Calculate the accuracy
  accuracy <- sum(pred_class == test.set$member_casual) / nrow(test.set)
  
  # Store the accuracy for this fold
  accuracy_results[i] <- accuracy
}

# Calculate the average accuracy across all folds
mean_accuracy <- mean(accuracy_results)
print(paste("Average accuracy across all folds: ", round(mean_accuracy, 4)))

# Optionally, visualize the final tree (trained on all data)
rpart.plot(Member_casual_tree, type = 3, extra = 1, main = "Final Classification Tree")


```



```{r}
library(rattle)

fancyRpartPlot(DurationModel)
```


```{r}
print(DurationModel$cptable)
```
From this table:

The lowest xerror occurs at cp = 0.0100, where xerror is 0.689.
However, the xerror is very close between cp = 0.0102 (0.697) and cp = 0.0100 (0.689). The difference is very small, suggesting that the improvement in error with cp = 0.0100 is marginal.
```{r}
plotcp(DurationModel)
```

Cross- validation:

```{r, results="markup"}
fold <- floor(runif(nrow(capital_bikeshare_clean),1,11)) 
capital_bikeshare_clean$fold <- fold
## the test set is just the first fold 
test.set <- capital_bikeshare_clean[capital_bikeshare_clean$fold == 1,] 
##exclude the first fold from the data here 
train.set <- capital_bikeshare_clean[capital_bikeshare_clean$fold != 1,]
DurationModel_fold <- rpart(ride_duration_mins ~ rideable_type + member_casual +distance_miles + start_time_period, data=train.set, method="anova" )
fancyRpartPlot(DurationModel_fold)
```

```{r}
pred <- predict(DurationModel_fold, test.set) 
plot(pred,test.set$ride_duration_mins, main="Predicted vs. Observed ride_duration_mins", xlab="Tree Predicted ride_duration_mins", ylab="Observed ride_duration_mins")
treefit_fold_error = sum(pred-test.set$ride_duration_mins)^2
print(treefit_fold_error)
```
The tree fold error is too large.

now considering only `rideable_type`, `member_casual`, `start_time_period`.
```{r, results="markup"}
DurationModel_fold2 <- rpart(ride_duration_mins ~ rideable_type + member_casual  + start_time_period, data=train.set, method="anova" )
fancyRpartPlot(DurationModel_fold2)
```

```{r}
pred2 <- predict(DurationModel_fold2, test.set) 
plot(pred2,test.set$ride_duration_mins, main="Predicted vs. Observed ride_duration_mins", xlab="Tree Predicted ride_duration_mins", ylab="Observed ride_duration_mins")
treefit_fold_error2 = sum(pred2-test.set$ride_duration_mins)^2
print(treefit_fold_error2)
```


The tree fold error is reduced  but still is large. But given that the test data is `r nrow(test.set)` the average error per prediction is `r treefit_fold_error2/nrow(test.set)`.

```{r}
summary(DurationModel_fold2)
```

To predict for a new data.

```{r}
new_data <- data.frame(
  rideable_type = factor("electric_bike"),
  member_casual = factor("casual"),
  start_time_period = factor("Morning")
)

# Predict the ride duration
predicted_duration <- predict(DurationModel_fold2, new_data)
predicted_duration
```




```{r}
rideable_typeLogit1 <- glm(rideable_type ~ member_casual , data = capital_bikeshare_clean, family = "binomial")

summary(rideable_typeLogit1)
```


```{r}
rideable_typeLogit2 <- glm(rideable_type ~ member_casual +distance_miles, data = capital_bikeshare_clean, family = "binomial")

summary(rideable_typeLogit2)
```


```{r}
rideable_typeLogit3 <- glm(rideable_type ~ member_casual *distance_miles, data = capital_bikeshare_clean, family = "binomial")

summary(rideable_typeLogit3)
```

```{r}
rideable_typeLogit4 <- glm(rideable_type ~ member_casual +distance_miles+ ride_duration_mins, data = capital_bikeshare_clean, family = "binomial")

summary(rideable_typeLogit4)
```


```{r}
rideable_typeLogit5 <- glm(rideable_type ~ member_casual *distance_miles* ride_duration_mins, data = capital_bikeshare_clean, family = "binomial")

summary(rideable_typeLogit5)
```


```{r}
rideable_typeLogit6 <- glm(rideable_type ~ member_casual +distance_miles+ ride_duration_mins+start_time_period, data = capital_bikeshare_clean, family = "binomial")

summary(rideable_typeLogit6)
```

```{r}
rideable_typeLogit7 <- glm(rideable_type ~ member_casual *distance_miles* ride_duration_mins*start_time_period, data = capital_bikeshare_clean, family = "binomial")

summary(rideable_typeLogit7)
```

```{r}
rideable_typeLogit8 <- glm(rideable_type ~ member_casual +distance_miles+ ride_duration_mins+start_time_period+day_type, data = capital_bikeshare_clean, family = "binomial")

summary(rideable_typeLogit8)
```

```{r}
anova_result <- anova(rideable_typeLogit1, rideable_typeLogit2, rideable_typeLogit3, rideable_typeLogit4, rideable_typeLogit5, rideable_typeLogit6, rideable_typeLogit7, rideable_typeLogit8, test = "Chisq")
print(anova_result)
```

NOTES:


Models with interaction terms (Models 3, 5, 7) tend to show a significant improvement in deviance, indicating that interactions between member_casual, distance_miles, ride_duration_mins, and start_time_period are important in explaining rideable_type.
Adding ride_duration_mins and distance_miles substantially improves model fit.
The variable start_time_period increases deviance when added in Model 6, suggesting it may not be a good predictor in this context.
Adding day_type in Model 8 also worsens the model, indicating it doesn't contribute to explaining rideable_type.
 Model 5 (with interactions between all three of member_casual, distance_miles, and ride_duration_mins) seems to be the best fit.
 


```{r}
rideable_typeLogit9 <- glm(rideable_type ~ member_casual +distance_miles+ ride_duration_mins+(member_casual *distance_miles), data = capital_bikeshare_clean, family = "binomial")
```
 
```{r}
 rideable_typeLogit10 <- glm(rideable_type ~ member_casual +distance_miles+ ride_duration_mins+(member_casual *distance_miles) + (member_casual*ride_duration_mins), data = capital_bikeshare_clean, family = "binomial")
```



```{r}
 rideable_typeLogit11 <- glm(rideable_type ~ member_casual +distance_miles+ ride_duration_mins+(member_casual *distance_miles) + (distance_miles*ride_duration_mins), data = capital_bikeshare_clean, family = "binomial")
```

```{r}
anova_result <- anova(rideable_typeLogit1, rideable_typeLogit2, rideable_typeLogit3, rideable_typeLogit4, rideable_typeLogit5, rideable_typeLogit6, rideable_typeLogit7, rideable_typeLogit8,rideable_typeLogit9,rideable_typeLogit10,rideable_typeLogit11, test = "Chisq")
print(anova_result)
```
Based on the anova results, checking for the vif of top five models
```{r}
vif(rideable_typeLogit5)
vif(rideable_typeLogit7)
vif(rideable_typeLogit10)
vif(rideable_typeLogit2)
```

Based on the VIF and ANOVA results, models that include both member_casual and distance_miles are preferable due to their significant improvements in model fit, as evidenced by the deviance reductions and highly significant p-values. 


```{r}
predicted_probs <- predict(rideable_typeLogit2, type = "response")

# Classify based on a threshold of 0.5
predicted_classes <- ifelse(predicted_probs > 0.5, "electric_bike", "classic_bike")
predicted_classes <- factor(predicted_classes, levels = c("electric_bike", "classic_bike"))
actual_classes <- factor(capital_bikeshare_clean$rideable_type, levels = c("electric_bike", "classic_bike"))

# Create the confusion matrix
conf_matrix <- confusionMatrix(predicted_classes, actual_classes)

# Print the confusion matrix and statistics
print(conf_matrix)

# Extract accuracy
accuracy <- conf_matrix$overall["Accuracy"]
print(paste("Accuracy:", round(accuracy, 4)))
```



The model's accuracy is 0.622, meaning it correctly predicted 62.2% of the cases. However, the model is biased towards predicting the electric_bike class, with 100% sensitivity (all electric_bike instances were correctly classified) and 0% specificity (no classic_bike instances were correctly predicted). The Kappa statistic is 0, indicating no agreement beyond chance. The Mcnemar's Test p-value is highly significant (<2e-16), suggesting a strong difference in prediction errors for the two classes. Balanced accuracy is 0.5, reflecting the imbalance in predictions.




electric_bike_data <- capital_bikeshare_clean[capital_bikeshare_clean$rideable_type == "electric_bike", ]
classic_bike_data <- capital_bikeshare_clean[capital_bikeshare_clean$rideable_type == "classic_bike", ]

# Sample 2000 random rows from each type
electric_bike_sample <- electric_bike_data[sample(nrow(electric_bike_data), 2000), ]
classic_bike_sample <- classic_bike_data[sample(nrow(classic_bike_data), 2000), ]

# Combine both samples into a new dataset
sampled_data <- rbind(electric_bike_sample, classic_bike_sample)



```{r}
 rideable_typeLogit_sample <- glm(rideable_type ~ member_casual +distance_miles+ride_duration_mins+start_time_period, data = sampled_data, family = "binomial")
vif(rideable_typeLogit_sample)
```

```{r}
predicted_probs <- predict(rideable_typeLogit_sample, type = "response")

# Classify based on a threshold of 0.5
predicted_classes <- ifelse(predicted_probs > 0.5, "electric_bike", "classic_bike")
predicted_classes <- factor(predicted_classes, levels = c("electric_bike", "classic_bike"))
actual_classes <- factor(sampled_data$rideable_type, levels = c("electric_bike", "classic_bike"))

# Create the confusion matrix
conf_matrix <- confusionMatrix(predicted_classes, actual_classes)

# Print the confusion matrix and statistics
print(conf_matrix)

# Extract accuracy
accuracy <- conf_matrix$overall["Accuracy"]
print(paste("Accuracy:", round(accuracy, 4)))
```



Accuracy: 0.623, meaning the model correctly predicted 62.3% of cases.
Kappa: 0.245, indicating slight agreement beyond chance.
Sensitivity (Recall for electric_bike): 0.694, meaning the model correctly predicted 69.4% of actual electric_bike cases.
Specificity: 0.551, meaning the model correctly predicted 55.1% of classic_bike cases.
Positive Predictive Value: 0.607, meaning 60.7% of predicted electric_bike cases were correct.
Negative Predictive Value: 0.643, meaning 64.3% of predicted classic_bike cases were correct.
Balanced Accuracy: 0.623, which is the average of sensitivity and specificity, indicating the model is fairly balanced in predicting both classes.